---
title: "Narratives to Explain<br>AI Models"
subtitle: "<br>Antwerp Center on Responsible AI<br>Research Day 2025"
author: "<br>Mateusz Cedro"
format:
  revealjs: 
    css: styles.css
    theme: [default]
    slide-number: true
    touch: true
    scrollable: true
    chalkboard: 
      buttons: false
    logo: images/ua_logo.jpg
    footer: "XAI Narratives -- Mateusz Cedro -- ACRAI Research Day 2025"
---

# Model explanations ‚Äì Why should we care?
## Data, Models, Predictions

<p>
<!--
  <img src="images/the_economist.jpg" 
       width="40%" 
       style="float: right; margin-left: 1em; margin-bottom: 1em;" />
-->
![](images/the_economist.jpg){ width=40% style="float: right; margin-left: 1em; margin-bottom: 1em;" }
</p>
<br>
<br>
<p>
  "Data is the new oil"<br>
  ‚Äî Clive Humby, 2006
</p>
<br>
<p>
  "AI is the new electricity"<br>
  ‚Äî Andrew Ng, 2023
</p>
 <br><br><br><br><br>
 However, not every model works as expected...

<br>
 <p><img src="images/why.png" width="100%"/></p>

<div style="clear: both;"></div>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 2em;">
  Source: Przemys≈Çaw Biecek, Explainable AI course at MIM UW 2022,<br>
  "Why Should I Trust You?": Explaining the Predictions of Any Classifier, (Ribeiro et al., 2016)<br>
  Unmasking Clever Hans predictors and assessing what machines really learn, (Lapuschkin et al., 2019)
  https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data
</div>


## The right to an explanation and the AI Act
<br><br>

- The Right to an Explanation (Recital 71 EU GDPR): 

"The data subject should have the right [...] **to obtain an explanation of the decision** reached after such assessment and to challenge the decision."
<br><br>

- AI Act (2024):

"The AI Act is part of a wider package of policy measures to support the development of trustworthy AI"

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 10em;">
  Source: Przemys≈Çaw Biecek, Explainable AI course at MIM UW 2022,<br>
  https://www.privacy-regulation.eu/en/recital-71-GDPR.htm,<br>
  https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32024R1689
</div>

# Explaining model ‚Äì Inspirations from Social Sciences
## Counterfactual Thinking (Roese, 1997)

<br>

Principles of Counterfactuals:

- Counterfactual thinking is activated automatically in response to negative affect
- Counterfactuals are mental representations of alternatives to the past and produce consequences that are both beneficial and aversive to the individual
- Counterfactuals produce negative affective consequences through a contrast-effect mechanism and positive inferential consequences through a causal-inference mechanism 
- The net effect of counterfactual thinking is beneficial.

<br><br>
Roese, N. J. (1997). Counterfactual thinking. **Psychological Bulletin**, 121(1), 133‚Äì148. https://doi.org/10.1037/0033-2909.121.1.133

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 6em;">
  Source: APA PsychNet, https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-2909.121.1.133
</div>


## Game-Theoretic Foundations (Shapley, 1953)
How to divide the reward **equitably**?

- Three parties A, B and C took part in the election.
- As a result of the election, parties:
    - A and B each have 49% representation in the parliament,
    - C has 2% representation.
- Let‚Äôs assume that A and C formed a government.
- How to fairly divide the prize (ministries)?
- What share of the prize should party C have?

Note that any two parties can form a government. In that case, should the prize for C be equal to or less than that for A?

<p><img src="images/shap_v_01.png" width="100%"/></p>


Formula for Shapley Values (Shapley, 1953) for fair payoff for player ùëó based on their **average marginal contribution**:

$$
\phi_j = \sum_{S \subseteq P / \{j\}}  \frac{|S|! (|P| - |S| - 1)!}{|P|!} (v(S \cup \{j\}) - v(S))
$$

where,

- Players and Coalitions: The set ùëÉ contains all players. The formula sums over all subsets ùëÜ that do not include player ùëó.
- Marginal Contribution: ùë£(ùëÜ u {ùëó}) ‚àí ùë£(ùëÜ) measures how much extra value player ùëó brings to the coalition ùëÜ.
- Weighting Factor: $$\frac{|S|! (|P| - |S| - 1)!}{|P|!}$$ a combinatorial term that accounts for all possible ways players can join a coalition in different orders.

<br>

The fair payoff (Shapley Values) for parties:

- A:  **41.167** (49% of votes)
- B:  **41.167** (49% of votes)
- C:  **17.667** (2% of votes)

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 6em;">
  Source: Przemys≈Çaw Biecek, Explainable AI course at MIM UW 2022,<br>
  Shapley, L. (1953) A Value for n-Person Games<br>
  Contributions to the Theory of Games II, Princeton University Press, Princeton, 307-317.<br>
https://doi.org/10.1515/9781400881970-018.
</div>


# eXplainable AI (XAI) in practice
## Counterfactual (CF) explanations

*What needs to change in your data, to change the decision?*

Example: Loan approval

<p><img src="images/cf_e.png" width="100%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: NICE: An Algorithm for Nearest Instance Counterfactual Explanations,<br>
  Brughmans et al., 2021
</div>

## SHAP (Lundberg & Lee, 2017)

Motivated by Shapley values (Shapley, 1953), SHAP treats:

- Features as players
- The fair payoff for each player as the feature importance

**Feature importance based on Shapley values: the average marginal contribution of a feature across all possible coalitions.**

<br>
Example: a SHAP explanation for a student predicted to fail mathematics.
<p><img src="images/shap_1.png" width="100%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: A Unified Approach to Interpreting Model Predictions (Lundberg & Lee, 2017),<br>
  Tell Me a Story! Narrative-Driven XAI with Large Language Models (Martens et al., 2023)
</div>

## Explanation on Graphs

GNNExplainer (Ying et al., 2019) to explain Graph Neural Networks:

- Subgraph: Identifies crucial connections to neighboring nodes
- Feature importance: Significance of node attributes

Example: GNNExplainer's explanation for an NBA player (node 379) predicted to have a 'Low Salary'.

<p><img src="images/gnnexp.png" width="60%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019),<br>
  GraphXAIN: Narratives to Explain Graph Neural Networks (Cedro & Martens, 2024)
</div>

# Confused?

## Current XAI Methods

<br>
"Inmates running the asylum"<br>
  ‚Äî Tim Miller, 2017

- AI researchers often create solutions tailored to their own needs rather than those of non-expert users (Miller, 2017)
- ‚ÄúThere is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones.‚Äù  (Bhatt et al., 2020)

# What Is a "Good" Explanation?

## Social sciences once again

<br>

- Computer-generated results should be **directly interpretable in natural language** (Michalski, 1983)
- Effective explanations **should enhance the alignment between the user‚Äôs mental model and AI model** (Kayande et al., 2009)

## Solution: Narratives!

<p><img src="images/communicate.png" width="80%"/></p>
<p><img src="images/communicate2.png" width="65%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: Using narratives and storytelling to communicate science with nonexperts audiences (Dahlstron, 2014),<br>
  How narrative functions in entertainment to communicate science (Kaplan & Dahlstron, 2017)
</div>

# XAI Narratives

## CFstories
Tell Me a Story! Narrative-Driven XAI with Large Language Models (Martens et al., 2023)

<p><img src="images/cfstory.png" width="75%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: Tell Me a Story! Narrative-Driven XAI with Large Language Models (Martens et al., 2023)
</div>

## SHAPstories

<p><img src="images/shapstory.png" width="80%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: Tell Me a Story! Narrative-Driven XAI with Large Language Models (Martens et al., 2023)
</div>

## GraphXAIN
GraphXAIN: Narratives to Explain Graph Neural Networks (Cedro & Martens, 2024)

<p><img src="images/graphxain_prez.png" width="100%"/></p>

<div style="font-size: 0.8em; color: gray; text-align: center; margin-top: 3em;">
  Source: GraphXAIN: Narratives to Explain Graph Neural Networks (Cedro & Martens, 2024)
</div>

## Evaluation of XAI Narratives

- CF- and SHAPstories (Martens et al., 2023)
<p><img src="images/xaistories.png" width="90%"/></p>

- GraphXAIN (Cedro & Martens, 2024)
<p><img src="images/xain_res.png" width="80%"/></p>

# Conclusion: 

<h2>**Narratives ‚Äì A Crucial Addition to Explainable AI**</h2>

# Thank you!
<h3>Happy to answer your questions!<h3>